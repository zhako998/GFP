{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Copyright 2022 IBM Corporation\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "     http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Features Extraction for Anti-Money Laudering\n",
    "\n",
    "The Snap ML GraphFeaturePreprocessor is a scikit-learn compatible preprocessor that enables scalable and real-time feature extraction from graph-structured data. It provides utilities for creating and updating in-memory graphs as well as extracting new features from these graphs. The goal of this example is to show how to use the API of this preprocessor. As input, we will use a synthethic dataset in tabular format where each row represents a financial transaction. For each transaction 4 features are available: transaction ID, source account ID, target accound ID and transaction timestamp. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Graph Feature Preprocessor from Snap ML\n",
    "from snapml import GraphFeaturePreprocessor\n",
    "\n",
    "# # Import other libraries\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon_laun_path = \"datasets/HI-Small_Trans.csv\"\n",
    "# mon_laun_df_init = pd.read_csv(mon_laun_path, nrows = 1000)\n",
    "mon_laun_df_init = pd.read_csv(mon_laun_path)\n",
    "\n",
    "mon_laun_df_init = mon_laun_df_init.sort_values(by = [\"Timestamp\"], ascending = True)\n",
    "\n",
    "# display(mon_laun_df_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = mon_laun_df_init['Receiving Currency'].unique()\n",
    "x2 = mon_laun_df_init['Payment Currency'].unique()\n",
    "x3 = mon_laun_df_init['Payment Format'].unique()\n",
    "\n",
    "x4 = pd.unique(mon_laun_df_init[[\"From Bank\", \"To Bank\"]].values.ravel('K'))\n",
    "x5 = pd.unique(mon_laun_df_init[[\"Account\", \"Account.1\"]].values.ravel('K'))\n",
    "x6 = mon_laun_df_init.loc[mon_laun_df_init['Is Laundering'] == 1]\n",
    "\n",
    "# print(x5)\n",
    "# print(len(x5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon_laun_df_init[\"transactionID\"] = mon_laun_df_init.index.astype(float)\n",
    "mon_laun_df_init[\"sourceAccount\"] = mon_laun_df_init[\"From Bank\"].astype(str) + \"_\" + mon_laun_df_init[\"Account\"]\n",
    "mon_laun_df_init[\"targetAccount\"] = mon_laun_df_init[\"To Bank\"].astype(str) + \"_\" + mon_laun_df_init[\"Account.1\"]\n",
    "\n",
    "mon_laun_df_init[\"Timestamp\"] = pd.to_datetime(mon_laun_df_init[\"Timestamp\"], format = '%Y/%m/%d %H:%M')\n",
    "# The output is in nanoseconds. Convert to seconds by deviding 10**9\n",
    "mon_laun_df_init[\"Timestamp_float\"] = mon_laun_df_init[\"Timestamp\"].astype('int64') // 10 ** 9\n",
    "# display(mon_laun_df_init)\n",
    "\n",
    "x7 = pd.unique(mon_laun_df_init[[\"sourceAccount\", \"targetAccount\"]].values.ravel('K'))\n",
    "# print(x7)\n",
    "# print(len(x7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing source and target with unique id\n",
    "unique_account_df = mon_laun_df_init[[\"sourceAccount\", \"targetAccount\"]]\n",
    "x = unique_account_df.stack()\n",
    "x[:] = x.factorize()[0]\n",
    "mon_laun_df_init = mon_laun_df_init.join(x.unstack().add_suffix('ID'))\n",
    "\n",
    "# Replacing currency with unique id\n",
    "unique_currency_df = mon_laun_df_init[[\"Receiving Currency\", \"Payment Currency\"]]\n",
    "x = unique_currency_df.stack()\n",
    "x[:] = x.factorize()[0]\n",
    "mon_laun_df_init = mon_laun_df_init.join(x.unstack().add_suffix('ID'))\n",
    "\n",
    "\n",
    "# Replacing payment format with unique id\n",
    "mon_laun_df_init['Payment FormatID'], mapping = mon_laun_df_init['Payment Format'].factorize()\n",
    "\n",
    "x8 = pd.unique(mon_laun_df_init[[\"sourceAccountID\", \"targetAccountID\"]].values.ravel('K'))\n",
    "# print(x8)\n",
    "# print(len(x8))\n",
    "\n",
    "x9 = pd.unique(mon_laun_df_init[[\"Receiving CurrencyID\", \"Payment CurrencyID\"]].values.ravel('K'))\n",
    "# print(x9)\n",
    "# print(len(x9))\n",
    "\n",
    "x10 = pd.unique(mon_laun_df_init[\"Payment Format\"].values.ravel('K'))\n",
    "# print(x10)\n",
    "# print(len(x10))\n",
    "\n",
    "# display(mon_laun_df_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon_laun_df = mon_laun_df_init.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mon_laun_df = mon_laun_df[mon_laun_df[\"sourceAccountID\"] != mon_laun_df[\"targetAccountID\"]]\n",
    "# display(mon_laun_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon_laun_df_labels = mon_laun_df[\"Is Laundering\"]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(mon_laun_df, mon_laun_df_labels, \n",
    "                                                    test_size = 0.2, \n",
    "                                                    random_state = 42, \n",
    "                                                    shuffle = False, \n",
    "                                                    stratify = None\n",
    "                                                   )\n",
    "# display(x_train)\n",
    "# display(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_float_columns = ['transactionID', 'Timestamp', \\\n",
    "                     \"sourceAccount\", \"Account\", \\\n",
    "                     \"targetAccount\", \"Account.1\", \\\n",
    "                     \"Receiving Currency\", \"Payment Currency\", \"Payment Format\"\n",
    "                    ]\n",
    "\n",
    "correct_columns_order = ['transactionID', 'sourceAccountID', 'targetAccountID', \\\n",
    "                         'Timestamp_float', \\\n",
    "                         \"Is Laundering\", \\\n",
    "                         \"From Bank\", \\\n",
    "                         \"To Bank\", \\\n",
    "                         \"Amount Received\", \"Receiving CurrencyID\", \\\n",
    "                         \"Amount Paid\", \"Payment CurrencyID\", \\\n",
    "                         \"Payment FormatID\"\n",
    "                        ]\n",
    "columns_for_vertex_stats = [3, 7, 8, 9, 10, 11]\n",
    "\n",
    "x_train_meta = x_train[non_float_columns]\n",
    "x_train = x_train[correct_columns_order]\n",
    "\n",
    "x_test_meta = x_test[non_float_columns]\n",
    "x_test = x_test[correct_columns_order]\n",
    "\n",
    "# display(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transactionID           float64\n",
      "sourceAccountID          object\n",
      "targetAccountID          object\n",
      "Timestamp_float           int64\n",
      "Is Laundering             int64\n",
      "From Bank                 int64\n",
      "To Bank                   int64\n",
      "Amount Received         float64\n",
      "Receiving CurrencyID     object\n",
      "Amount Paid             float64\n",
      "Payment CurrencyID       object\n",
      "Payment FormatID          int64\n",
      "dtype: object\n",
      "(4062676, 12)\n"
     ]
    }
   ],
   "source": [
    "x_train = pd.DataFrame(x_train)\n",
    "print(x_train.dtypes)\n",
    "print(x_train.shape)\n",
    "x_train = np.ascontiguousarray(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transactionID           float64\n",
      "sourceAccountID          object\n",
      "targetAccountID          object\n",
      "Timestamp_float           int64\n",
      "Is Laundering             int64\n",
      "From Bank                 int64\n",
      "To Bank                   int64\n",
      "Amount Received         float64\n",
      "Receiving CurrencyID     object\n",
      "Amount Paid             float64\n",
      "Payment CurrencyID       object\n",
      "Payment FormatID          int64\n",
      "dtype: object\n",
      "(1015669, 12)\n"
     ]
    }
   ],
   "source": [
    "x_test = pd.DataFrame(x_test)\n",
    "print(x_test.dtypes)\n",
    "print(x_test.shape)\n",
    "x_test = np.ascontiguousarray(x_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Here we assume that the user has access to a set of (labeled) transactions with raw features which could be used to train a machine learning (ML) model, e.g., for fraud detection. The user will extract graph features using the Graph Features Preprocessor which will be added to the initial raw features present in the transactions. The enriched set of features will be used to train an ML model. The main steps associated with this use case are shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 6, 11, 16, 21, 26]\n"
     ]
    }
   ],
   "source": [
    "# The following dictionary defines the configuration parameters of the Graph Feature Preprocessor\n",
    "\n",
    "# transaction (edgeID), source-destination (vertex), Timestamp --> first 4 columns\n",
    "    # other raw features, and new graph-based and vertex features\n",
    "\n",
    "# timestamp --> is also needed for sorting input data in increasing order\n",
    "# unit of time is the same as the one used for timestamps of edges -- seconds\n",
    "\n",
    "# batch size of 128 -- ??\n",
    "\n",
    "#################################################\n",
    "hw = 5  # histogram size\n",
    "hw_bin_range = 5 # histogram bin size\n",
    "\n",
    "# Array used for specifying the bins of the pattern histogram\n",
    "histogram_range_init = [y * hw_bin_range + hw_bin_range + 1 for y in range(-1, hw)]\n",
    "print(histogram_range_init)\n",
    "      \n",
    "vertex_stats_feats_types = [i for i in range(11)]\n",
    "\n",
    "# PAPER: not specified values --> vertex_stats_tw, time_window, max_no_edges\n",
    "params = {\n",
    "    \"num_threads\": 12,            # number of software threads to be used (important for performance)\n",
    "    \"vertex_stats\": True,         # produce vertex statistics\n",
    "\n",
    "    \"vertex_stats_tw\": 12 * 60 * 60,        # time window to consider\n",
    "    \"time_window\": 12 * 60 * 60,            # time window used if no pattern was specified\n",
    "                                            # transactions in largest time - time window\n",
    "    \n",
    "    \"max_no_edges\": 10,           # limit number of edges in detecting simple cycles\n",
    "                                  # -1 means it is defined only using the time window\n",
    "    \n",
    "    \"vertex_stats_cols\": columns_for_vertex_stats,     \n",
    "    # produce vertex statistics using the selected input columns\n",
    "    # NOTE: Columns of the input numpy array used for generating vertex statistics features\n",
    "    # NOTE: money amount, currency, etc. -- of source and target\n",
    "    \n",
    "    # features: 0:fan,1:deg,2:ratio,3:avg,4:sum,5:min,6:max,7:median,8:var,9:skew,10:kurtosis\n",
    "    \"vertex_stats_feats\": vertex_stats_feats_types, # PAPER: computed based on Amount and Timestamp??\n",
    "    \n",
    "    # fan in/out parameters -- PAPER: NOT USED\n",
    "    \"fan\": True,\n",
    "    \"fan_tw\": 12 * 60 * 60, # seconds to hours (12 or 24)\n",
    "    \"fan_bins\": histogram_range_init,\n",
    "    \n",
    "    # in/out degree parameters -- PAPER: NOT USED\n",
    "    \"degree\": True,\n",
    "    \"degree_tw\": 12 * 60 * 60, # seconds to hours (12 or 24)\n",
    "    \"degree_bins\": histogram_range_init,\n",
    "    \n",
    "    # scatter gather parameters\n",
    "    \"scatter-gather\": True,\n",
    "    \"scatter-gather_tw\": 6 * 60 * 60, # seconds to hours\n",
    "    \"scatter-gather_bins\": histogram_range_init,\n",
    "\n",
    "    # # gather scatter parameters -- UNSUPPORTED\n",
    "    # \"gather-scatter\": True,\n",
    "    # \"gather-scatter_tw\": 6 * 60 * 60, # seconds to hours\n",
    "    # \"gather-scatter_bins\": histogram_range_init,\n",
    "    \n",
    "    # temporal cycle parameters\n",
    "    \"temp-cycle\": True,\n",
    "    \"temp-cycle_tw\": 12 * 60 * 60, # seconds to hours (12 or 24)\n",
    "    \"temp-cycle_bins\": histogram_range_init,\n",
    "    \n",
    "    # length-constrained simple cycle parameters\n",
    "    \"lc-cycle\": True,\n",
    "    \"lc-cycle_tw\": 12 * 60 * 60, # seconds to hours (12 or 24)\n",
    "    \"lc-cycle_bins\": histogram_range_init,\n",
    "    \"lc-cycle_len\": 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a graph feature preprocessor \n",
      "Setting the parameters of the graph feature preprocessor \n",
      "Graph feature preprocessor parameters:  {\n",
      "    \"num_threads\": 12,\n",
      "    \"time_window\": 43200,\n",
      "    \"max_no_edges\": 10,\n",
      "    \"vertex_stats\": true,\n",
      "    \"vertex_stats_tw\": 43200,\n",
      "    \"vertex_stats_cols\": [\n",
      "        3,\n",
      "        7,\n",
      "        8,\n",
      "        9,\n",
      "        10,\n",
      "        11\n",
      "    ],\n",
      "    \"vertex_stats_feats\": [\n",
      "        0,\n",
      "        1,\n",
      "        2,\n",
      "        3,\n",
      "        4,\n",
      "        5,\n",
      "        6,\n",
      "        7,\n",
      "        8,\n",
      "        9,\n",
      "        10\n",
      "    ],\n",
      "    \"fan\": true,\n",
      "    \"fan_tw\": 43200,\n",
      "    \"fan_bins\": [\n",
      "        1,\n",
      "        6,\n",
      "        11,\n",
      "        16,\n",
      "        21,\n",
      "        26\n",
      "    ],\n",
      "    \"degree\": true,\n",
      "    \"degree_tw\": 43200,\n",
      "    \"degree_bins\": [\n",
      "        1,\n",
      "        6,\n",
      "        11,\n",
      "        16,\n",
      "        21,\n",
      "        26\n",
      "    ],\n",
      "    \"scatter-gather\": true,\n",
      "    \"scatter-gather_tw\": 21600,\n",
      "    \"scatter-gather_bins\": [\n",
      "        1,\n",
      "        6,\n",
      "        11,\n",
      "        16,\n",
      "        21,\n",
      "        26\n",
      "    ],\n",
      "    \"temp-cycle\": true,\n",
      "    \"temp-cycle_tw\": 43200,\n",
      "    \"temp-cycle_bins\": [\n",
      "        1,\n",
      "        6,\n",
      "        11,\n",
      "        16,\n",
      "        21,\n",
      "        26\n",
      "    ],\n",
      "    \"lc-cycle\": true,\n",
      "    \"lc-cycle_tw\": 43200,\n",
      "    \"lc-cycle_len\": 10,\n",
      "    \"lc-cycle_bins\": [\n",
      "        1,\n",
      "        6,\n",
      "        11,\n",
      "        16,\n",
      "        21,\n",
      "        26\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Create a Graph Feature Preprocessor, set its configuration using the above dictionary and verify it\n",
    "\n",
    "print(\"Creating a graph feature preprocessor \")\n",
    "gp = GraphFeaturePreprocessor()\n",
    "\n",
    "print(\"Setting the parameters of the graph feature preprocessor \")\n",
    "gp.set_params(params)\n",
    "\n",
    "print(\"Graph feature preprocessor parameters: \", json.dumps(gp.get_params(), indent = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriching the transactions with new graph features \n",
      "train wait 1: 1103.5973081588745 sec\n",
      "(4062676, 258)\n"
     ]
    }
   ],
   "source": [
    "print(\"Enriching the transactions with new graph features \")\n",
    "# the fit_transform and transform functions are equivalent\n",
    "# these functions can run on single transactions or on batches of transactions\n",
    "\n",
    "start = time.time()\n",
    "x_train_to_graph = gp.transform(x_train)\n",
    "end = time.time()\n",
    "print(\"train wait 1: \" + str(end - start) + \" sec\")\n",
    "\n",
    "print(x_train_to_graph.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test wait 1: 24.258920907974243 sec\n",
      "test wait 2: 178.39623188972473 sec\n",
      "(1015669, 258)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "gp.fit(x_train)\n",
    "# gp.partial_fit(x_train) # updating the graph\n",
    "end = time.time()\n",
    "print(\"test wait 1: \" + str(end - start) + \" sec\")\n",
    "\n",
    "start = time.time()\n",
    "x_test_to_graph = gp.transform(x_test)\n",
    "# x_test_to_graph = gp.fit_transform(x_test) # fit + transform \n",
    "end = time.time()\n",
    "print(\"test wait 2: \" + str(end - start) + \" sec\")\n",
    "\n",
    "print(x_test_to_graph.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a helper function to inspect the newly generated graph-based features for a given transaction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_enriched_transaction(transaction, params):\n",
    "    # add raw features names\n",
    "    colnames = correct_columns_order.copy()\n",
    "    \n",
    "    # add features names for the graph patterns\n",
    "    for pattern in ['fan', 'degree', 'scatter-gather', 'temp-cycle', 'lc-cycle']:\n",
    "        if pattern in params:\n",
    "            if params[pattern]:\n",
    "                bins = len(params[pattern + '_bins'])\n",
    "                if pattern in ['fan', 'degree']:\n",
    "                    for i in range(bins - 1):\n",
    "                        colnames.append(pattern + \"_in_bins_\" + str(params[pattern + '_bins'][i]) + \n",
    "                                        \"-\" + str(params[pattern + '_bins'][i + 1]))\n",
    "                    colnames.append(pattern + \"_in_bins_\" + str(params[pattern +'_bins'][i + 1]) + \"-inf\")\n",
    "\n",
    "                    for i in range(bins - 1):\n",
    "                        colnames.append(pattern + \"_out_bins_\" + str(params[pattern + '_bins'][i]) + \n",
    "                                        \"-\" + str(params[pattern + '_bins'][i + 1]))\n",
    "                    colnames.append(pattern + \"_out_bins_\" + str(params[pattern + '_bins'][i + 1]) + \"-inf\")\n",
    "                else:\n",
    "                    for i in range(bins - 1):\n",
    "                        colnames.append(pattern + \"_bins_\" + str(params[pattern + '_bins'][i]) + \n",
    "                                        \"-\" + str(params[pattern + '_bins'][i + 1]))\n",
    "                    colnames.append(pattern + \"_bins_\" + str(params[pattern + '_bins'][i + 1]) + \"-inf\")\n",
    "\n",
    "    vert_feat_names = [\"fan\",\"deg\",\"ratio\",\"avg\",\"sum\",\"min\",\"max\",\"median\",\"var\",\"skew\",\"kurtosis\"]\n",
    "\n",
    "    # add features names for the vertex statistics\n",
    "    for orig in ['source', 'dest']:\n",
    "        for direction in ['out', 'in']:\n",
    "            # add fan, deg, and ratio features\n",
    "            for k in [0, 1, 2]:\n",
    "                if k in params[\"vertex_stats_feats\"]:\n",
    "                    feat_name = orig + \"_\" + vert_feat_names[k] + \"_\" + direction\n",
    "                    colnames.append(feat_name)\n",
    "            for col in params[\"vertex_stats_cols\"]:\n",
    "                # add avg, sum, min, max, median, var, skew, and kurtosis features\n",
    "                for k in [3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "                    if k in params[\"vertex_stats_feats\"]:\n",
    "                        feat_name = orig + \"_\" + vert_feat_names[k] + \"_col\" + str(col) + \"_\" + direction\n",
    "                        colnames.append(feat_name)\n",
    "\n",
    "    df = pd.DataFrame(transaction, columns = colnames)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched transactions: \n",
      "(4062676, 258)\n",
      "(1015669, 258)\n"
     ]
    }
   ],
   "source": [
    "print(\"Enriched transactions: \")\n",
    "df_train_enriched = print_enriched_transaction(x_train_to_graph, gp.get_params())\n",
    "df_test_enriched = print_enriched_transaction(x_test_to_graph, gp.get_params())\n",
    "\n",
    "print(df_train_enriched.shape)\n",
    "print(df_test_enriched.shape)\n",
    "\n",
    "# display(df_train_enriched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This newly enriched set of transactions can now be used to train a ML model. Once trained, the model can be used for prediction (e.g., detect anomalies) on new (unlabeled) transactions. The main steps associated with this use case is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4062676, 266)\n",
      "(1015669, 266)\n"
     ]
    }
   ],
   "source": [
    "df_xgboost_train_data = pd.merge(x_train_meta, df_train_enriched, on = 'transactionID')\n",
    "df_xgboost_test_data = pd.merge(x_test_meta, df_test_enriched, on = 'transactionID')\n",
    "\n",
    "output_column_order = ['transactionID', 'Is Laundering', \\\n",
    "                       'Timestamp', 'Timestamp_float', \\\n",
    "                       'sourceAccountID', 'sourceAccount', 'From Bank', 'Account', \\\n",
    "                       'targetAccountID', 'targetAccount', 'To Bank', 'Account.1', \\\n",
    "                       'Amount Received', 'Receiving Currency', 'Receiving CurrencyID', \\\n",
    "                       'Amount Paid', 'Payment Currency', 'Payment CurrencyID', \\\n",
    "                       'Payment Format', 'Payment FormatID'\n",
    "                        ]\n",
    "for col in df_xgboost_train_data.columns[len(output_column_order):]:\n",
    "    output_column_order.append(col)\n",
    "    \n",
    "df_xgboost_train_data = df_xgboost_train_data[output_column_order]\n",
    "df_xgboost_test_data = df_xgboost_test_data[output_column_order]\n",
    "\n",
    "print(df_xgboost_train_data.shape)\n",
    "print(df_xgboost_test_data.shape)\n",
    "\n",
    "# display(df_xgboost_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xgboost_train_data.to_csv('datasets/merged_dataset_train.csv', index = False)\n",
    "df_xgboost_test_data.to_csv('datasets/merged_dataset_test.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the enriched transactions can be used as input to the ML model previously trained. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
